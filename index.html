<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation.">
  <meta name="keywords" content="imitation learning, force-control, mobile manipulation, robots ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Hongtao Wu,
            </span>
            <span class="author-block">
              Ya Jing,
            </span>
            <span class="author-block">
              Chilam Cheang,
            </span>
            <span class="author-block">
              Guangzeng Chen,
            </span>
            <span class="author-block">
              Jiafeng Xu,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xinghang Li,
            </span>
            <span class="author-block">
              Minghuan Liu,
            </span>
            <span class="author-block">
              Hang Li,
            </span>
            <span class="author-block">
              Tao Kong
            </span>
          </div>
          <br>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ByteDance Research &nbsp;&nbsp;</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item transport_tomato">
          <video poster="" id="transport_tomato" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/transport_tomato.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item close_drawer">
          <video poster="" id="close_drawer" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/close_drawer.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item transport_broccoli">
          <video poster="" id="transport_broccoli" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/transport_broccoli.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item calvin_0">
          <video poster="" id="calvin_0" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/calvin_0.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item calvin_1">
          <video poster="" id="calvin_1" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/calvin_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item calvin_2">
          <video poster="" id="calvin_2" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/calvin_2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item calvin_3">
          <video poster="" id="calvin_3" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/calvin_3.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. 
            In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. 
            We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation.
            GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. 
            It predicts robot actions as well as future images in an end-to-end manner.
            Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset.
            We perform extensive experiments on the challenging CALVIN benchmark and a real robot.
            On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%.
            In the zero-shot generalization setting, GR-1 improves the success rate from 53.3% to 85.4%.
            In real robot experiments, \ourmethod also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects.
            We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Method</h2>
        <img id="framework" width="70%" src="static/images/overview.jpg">
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <br>
          <p>
            GR-1 is first pre-trained on the task of video prediction with a large-scale video dataset.
            It is then finetuned on robot data to learn multi-task visual robot manipulation.
            GR-1 is a simple GPT-style transformer which is able to take different modalities as inputs and outputs future images and actions.
            The language input is encoded via CLIP text encoder.
            Visual inputs are encoded via a vision transformer which has been pretrained with MAE.
            Robot states, which contains the 6D pose of the robot end-effector and a binary status of the gripper, are encoded via a MLP.
          </p>
        </div>
        </h3>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- main result -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Results on CALVIN Benchmark</h2>
        <img id="calvin_result" width="70%" src="static/images/calvin_results.png">
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <p>
            <br>
            <a href="http://calvin.cs.uni-freiburg.de/"><b>CALVIN</b></a> is a challenging benchmark fosuing on learning language-conditioned policy for long-horizon robot manipulation.
            It contains 34 tasks and 4 different environments A, B, C, and D.
            We perform experiments on CALVIN in four settings<br>
            &nbsp; 1). Long-Horizon Multi-Task Learning <br>
            &nbsp; 2). Zero-Shot Generalization to Unseen Scenes <br>
            &nbsp; 3). Small Data <br>
            &nbsp; 4). Zero-Shot Generalization to Unseen Languages <br>
            <br>
            <b>Long-Horizon Multi-Task Learning (ABCD->D)</b> <br>
            In this setting, the robot is trained with data collected from environments A, B, C, and D and evaluated in environment D.
            During the evaluation, the robot is instructed to perform 5 tasks in a row. 
            This helps evaluate the capability of long-horizon multi-tasking.<br>
            <br>
            <b>Zero-Shot Generalization to Unseen Scenes (ABC->D)</b><br>
            The robot is trained with data collected from environment A, B, and C and evaluated in environment D, which is unseen during training.<br>
            <br>
            <b>Small Data</b><br>
            Robot data is expensive.
            To evaluate the capability of learning from small data, we sampled 10% data from the training data of the ABCD->D split for training.
            Specifically, we sampled 66 trajectories for each of the 34 tasks, i.e. 2244 trajectories in total.<br>
            <br>
            <b>Zero-Shot Generalization to Unseen Languages</b><br>
            We leveraged GPT-4 to generate synonymous instructions for each of the 34 tasks and used these unseen language instructions during evaluation.<br>
            <br>
            GR-1 showcases strong performance and outperforms the baseline methods in all four settings.
            In particular, GR-1 shows powerful zero-shot generealization capability and possesses strong data efficiency.<br>
          </p>
        </div>
        </h3>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- main result -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Results on Real Robot Experiments</h2>
        <img id="robot_result" width="70%" src="static/images/real_robot.png">
        <img id="robot_result" width="70%" src="static/images/real_robot_results.png">
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <br>
          <p>
          We also performed real robot experiments to evaluate how GR-1 works in the real world.
          We first performed object transportation experiments.
          We evaluated on three settings: Seen Objects, Unseen Instances (unseen object instances of seen instances), and Unseen Categories (unseen objects of unseen categories).
          Example instructions include "put the broccoli onto the plate".
          We also performed experiments on articulated object manipulation.
          Example instructions include "open the drawer".
          GR-1 outperforms baseline methods and showcase strong potentials in zero-shot generalization to unseen objects.
          </p>
        </div>
        </h3>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p>
            The website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
  </div>
</footer>

</body>
</html>
