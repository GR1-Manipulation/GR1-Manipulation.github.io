<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation.">
  <meta name="keywords" content="imitation learning, force-control, mobile manipulation, robots ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GR1: Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Hongtao Wu,
            </span>
            <span class="author-block">
              Ya Jing,
            </span>
            <span class="author-block">
              Chilam Cheang,
            </span>
            <span class="author-block">
              Guangzeng Chen,
            </span>
            <span class="author-block">
              Jiafeng Xu,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xinghang Li,
            </span>
            <span class="author-block">
              Minghuan Liu,
            </span>
            <span class="author-block">
              Hang Li
            </span>
            <span class="author-block">
              Tao Kong
            </span>
          </div>
          <br>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ByteDance Research &nbsp;&nbsp;</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item seen_1">
          <video poster="" id="seen_1" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/demo/seen_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item unseen_instance_2">
          <video poster="" id="unseen_instance_2" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/demo/unseen_instance_2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item unseen_category_1">
          <video poster="" id="unseen_category_1" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/demo/unseen_category_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item open_drawer">
          <video poster="" id="open_drawer" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/demo/open_drawer.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. 
             We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation.
             GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset.
            We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. When trained on 10% data of the full dataset, GR-1 achieves a success rate of 77.8%, while the best baseline method achieves 66.8%. In the zero-shot generalization setting, GR-1 improves the success rate from 53.3% to 85.4%. 
            In real robot experiments, GR-1 also outperforms the comparing baseline method. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Code will be made available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
      </div>
    </div>
    <!-- /Paper video. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Method</h2>
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <p>
            TODO
          </p>
        </div>
        </h3>
        <img id="framework" width="70%" src="static/images/overview_rebuttal.jpg">
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- main result -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Results</h2>
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <p>
            TODO
          </p>
        </div>
        </h3>
        <img id="main_result" width="100%" src="">
      </div>
    </div>
  </div>
</section>


<section class="section">
  <!-- Concurrent Work. -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>
        <div class="content has-text-justified">
          TODO
        </div>
      </div>
    </div>
  </div>
  <!--/ Concurrent Work. -->
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p>
            The website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
  </div>
</footer>

</body>
</html>
